"""Main CLI for poc-api-spec-rag."""

import click
from pathlib import Path
from src.core import settings


@click.group()
@click.version_option(version="0.1.0")
def cli():
    """RAG-based API Specification Assistant

    OpenAPI ëª…ì„¸ì„œì—ì„œ ì •í™•í•œ cURL ëª…ë ¹ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    pass


@cli.command()
@click.argument("spec_file", type=click.Path(exists=True))
@click.option("--force", is_flag=True, help="ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸°")
def ingest(spec_file: str, force: bool):
    """OpenAPI ëª…ì„¸ì„œë¥¼ ì¸ì œìŠ¤íŠ¸í•©ë‹ˆë‹¤.

    Examples:
        $ python -m src.main ingest ./specs/payment-api.yaml
        $ python -m src.main ingest ./specs/user-api.json --force
    """
    from src.ingestion import OpenAPIParser, EndpointChunker, OllamaEmbedder, ChromaIndexer

    click.echo(f"ğŸ“¥ Ingesting OpenAPI spec: {spec_file}")

    spec_path = Path(spec_file)

    if force:
        click.echo("âš ï¸  Force mode: ê¸°ì¡´ ë°ì´í„°ë¥¼ ë®ì–´ì”ë‹ˆë‹¤")

    try:
        # 1. OpenAPI íŒŒì‹±
        click.echo("\n[1/4] Parsing OpenAPI spec...")
        parser = OpenAPIParser()
        spec = parser.parse_file(spec_path)
        parser.validate_spec(spec)
        click.echo(f"âœ… Parsed: {len(spec.paths)} paths")

        # 2. ì—”ë“œí¬ì¸íŠ¸ ì²­í‚¹
        click.echo("\n[2/4] Chunking endpoints...")
        chunker = EndpointChunker()
        chunks = chunker.chunk_spec(spec)
        click.echo(f"âœ… Created {len(chunks)} chunks")

        # 3. ì„ë² ë”© ìƒì„±
        click.echo("\n[3/4] Generating embeddings...")
        embedder = OllamaEmbedder()

        # ì„ë² ë”© ì°¨ì› í™•ì¸
        dim = embedder.get_embedding_dimension()
        click.echo(f"   Embedding model: {embedder.model} ({dim} dim)")

        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        embeddings = embedder.embed_chunks(chunks)
        click.echo(f"âœ… Generated {len(embeddings)} embeddings")

        # 4. ChromaDB ì €ì¥
        click.echo("\n[4/4] Indexing to ChromaDB...")
        indexer = ChromaIndexer(reset=force)
        indexer.index_chunks(chunks, embeddings)

        # ì €ì¥ ê²°ê³¼ í™•ì¸
        info = indexer.get_collection_info()
        click.echo(f"âœ… Indexed to collection: {info['name']}")
        click.echo(f"   Total documents: {info['count']}")

        click.echo("\n" + "=" * 60)
        click.echo("âœ… Ingestion completed successfully!")
        click.echo("=" * 60)

    except Exception as e:
        click.echo(f"\nâŒ Ingestion failed: {e}", err=True)
        raise click.Abort()


@cli.command()
@click.argument("query")
@click.option("--top-k", default=5, help="ê²€ìƒ‰í•  ì—”ë“œí¬ì¸íŠ¸ ê°œìˆ˜")
@click.option("--verbose", "-v", is_flag=True, help="ìƒì„¸ ì¶œë ¥")
@click.option("--validate", is_flag=True, help="ê²€ì¦ í™œì„±í™”")
def query(query: str, top_k: int, verbose: bool, validate: bool):
    """ìì—°ì–´ ì§ˆì˜ë¡œ cURL ëª…ë ¹ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Examples:
        $ python -m src.main query "ê²°ì œ ìŠ¹ì¸ API curl ë§Œë“¤ì–´ì¤˜"
        $ python -m src.main query "ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ" --top-k 3 --verbose
        $ python -m src.main query "ê²°ì œ ìŠ¹ì¸" --validate
    """
    from src.retrieval import QueryProcessor, VectorSearcher, LLMReranker
    from src.generation import PromptBuilder, OllamaLLMClient, OutputParser
    from src.validation import CurlValidator, SpecValidator, ConfidenceScorer

    click.echo(f"ğŸ” Query: {query}")

    if verbose:
        click.echo(f"\nì„¤ì •:")
        click.echo(f"  - Embedding model: {settings.OLLAMA_EMBEDDING_MODEL}")
        click.echo(f"  - LLM model: {settings.OLLAMA_LLM_MODEL}")
        click.echo(f"  - Top-K: {top_k}")
        click.echo(f"  - Similarity threshold: {settings.SIMILARITY_THRESHOLD}")

    try:
        # [1/4] Retrieval Pipeline
        click.echo("\n[1/4] Retrieval Pipeline...")
        processor = QueryProcessor()
        searcher = VectorSearcher()
        reranker = LLMReranker()

        # ì§ˆì˜ ì²˜ë¦¬
        query_req = processor.process_query(query, top_k=top_k)
        if verbose:
            click.echo(f"  - Filters: {query_req.filters}")

        # ì„ë² ë”© ìƒì„±
        query_embedding = processor.embed_query(query)

        # ë²¡í„° ê²€ìƒ‰
        retrieval_resp = searcher.search(query_embedding, query_req)
        if not retrieval_resp.results:
            click.echo("âŒ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return

        if verbose:
            click.echo(f"  - Found {len(retrieval_resp.results)} endpoints")

        # LLM ì¬ì •ë ¬
        reranked_results = reranker.rerank(query, retrieval_resp.results, top_n=1)
        top_result = reranked_results[0]

        if verbose:
            click.echo(f"  - Top result: {top_result.chunk.metadata.method} {top_result.chunk.metadata.endpoint}")
            click.echo(f"  - Similarity: {top_result.similarity_score:.3f}")

        # [2/4] Generation Pipeline
        click.echo("\n[2/4] Generation Pipeline...")
        builder = PromptBuilder()
        llm = OllamaLLMClient()
        parser = OutputParser()

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        gen_req = builder.build_prompt(query, [top_result.chunk])
        user_prompt = builder._build_user_prompt(gen_req.query, gen_req.retrieved_chunks)

        # LLM ìƒì„±
        if verbose:
            click.echo(f"  - Calling {settings.OLLAMA_LLM_MODEL}...")
        llm_output = llm.generate(gen_req.system_prompt, user_prompt)

        # ì¶œë ¥ íŒŒì‹±
        source_endpoint = f"{top_result.chunk.metadata.method} {top_result.chunk.metadata.endpoint}"
        gen_resp = parser.parse_curl_response(llm_output, source_endpoint)

        if not gen_resp.curl_command.command:
            click.echo("\nâŒ cURL ìƒì„± ì‹¤íŒ¨")
            if gen_resp.curl_command.explanation:
                click.echo(f"ì‚¬ìœ : {gen_resp.curl_command.explanation}")
            if gen_resp.warnings:
                click.echo("\nâš ï¸  ê²½ê³ :")
                for warn in gen_resp.warnings:
                    click.echo(f"  - {warn}")
            return

        # [3/4] Validation Pipeline (ì˜µì…˜)
        confidence = None
        if validate:
            click.echo("\n[3/4] Validation Pipeline...")
            curl_val = CurlValidator()
            spec_val = SpecValidator()
            scorer = ConfidenceScorer()

            # cURL ë¬¸ë²• ê²€ì¦
            syntax_valid, syntax_errors = curl_val.validate(gen_resp.curl_command.command)
            if verbose and syntax_errors:
                click.echo(f"  - Syntax errors: {syntax_errors}")

            # ëª…ì„¸ ì¤€ìˆ˜ ê²€ì¦
            curl_components = curl_val.get_curl_components(gen_resp.curl_command.command)
            spec_valid, spec_warnings, spec_completeness = spec_val.validate(
                curl_components, top_result.chunk
            )

            if verbose and spec_warnings:
                click.echo(f"  - Spec warnings: {spec_warnings}")

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = scorer.calculate_score(
                similarity=top_result.similarity_score,
                spec_completeness=spec_completeness,
                syntax_valid=syntax_valid,
                spec_valid=spec_valid,
            )

            if verbose:
                click.echo(f"  - Confidence: {confidence.level} ({confidence.overall:.2f})")

        # [4/4] ê²°ê³¼ ì¶œë ¥
        click.echo("\n" + "=" * 60)
        click.echo("ìƒì„±ëœ cURL:")
        click.echo("=" * 60)
        click.echo(gen_resp.curl_command.command)

        if gen_resp.curl_command.explanation:
            click.echo(f"\nì„¤ëª…:")
            click.echo(gen_resp.curl_command.explanation)

        if gen_resp.curl_command.required_params:
            click.echo(f"\ní•„ìˆ˜ ì…ë ¥:")
            for param in gen_resp.curl_command.required_params:
                click.echo(f"  - {param}")

        if gen_resp.curl_command.expected_responses:
            click.echo(f"\nì˜ˆìƒ ì‘ë‹µ:")
            for code, desc in gen_resp.curl_command.expected_responses.items():
                click.echo(f"  - {code}: {desc}")

        # ì‹ ë¢°ë„ í‘œì‹œ (--validate ì˜µì…˜)
        if confidence:
            click.echo(f"\nì‹ ë¢°ë„: {confidence.level.upper()}")
            click.echo(f"  - ì „ì²´ ì ìˆ˜: {confidence.overall:.2f}")
            click.echo(f"  - ê²€ìƒ‰ ìœ ì‚¬ë„: {confidence.similarity:.2f}")
            click.echo(f"  - ëª…ì„¸ ì™„ì„±ë„: {confidence.spec_completeness:.2f}")
            click.echo(f"  - ê²€ì¦ í†µê³¼: {'ì˜ˆ' if confidence.validation_passed else 'ì•„ë‹ˆì˜¤'}")

        # ê²½ê³  ë©”ì‹œì§€
        if gen_resp.warnings:
            click.echo("\nâš ï¸  ê²½ê³ :")
            for warn in gen_resp.warnings:
                click.echo(f"  - {warn}")

        click.echo("\n" + "=" * 60)

    except Exception as e:
        click.echo(f"\nâŒ Query failed: {e}", err=True)
        if verbose:
            import traceback
            click.echo(traceback.format_exc(), err=True)
        raise click.Abort()


@cli.command()
def info():
    """ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    click.echo("=" * 60)
    click.echo("API Spec RAG - System Information")
    click.echo("=" * 60)

    click.echo(f"\nğŸ“‚ Directories:")
    click.echo(f"  Project Root: {settings.PROJECT_ROOT}")
    click.echo(f"  Data Dir: {settings.DATA_DIR}")
    click.echo(f"  Specs Dir: {settings.SPECS_DIR}")
    click.echo(f"  ChromaDB Dir: {settings.CHROMA_DB_DIR}")

    click.echo(f"\nğŸ¤– Models:")
    click.echo(f"  Embedding: {settings.OLLAMA_EMBEDDING_MODEL} (768 dim)")
    click.echo(f"  LLM: {settings.OLLAMA_LLM_MODEL}")
    click.echo(f"  Ollama URL: {settings.OLLAMA_BASE_URL}")

    click.echo(f"\nâš™ï¸  RAG Settings:")
    click.echo(f"  Top-K: {settings.TOP_K}")
    click.echo(f"  Similarity Threshold: {settings.SIMILARITY_THRESHOLD}")
    click.echo(f"  High Confidence Threshold: {settings.HIGH_CONFIDENCE_THRESHOLD}")

    click.echo("\n" + "=" * 60)


@cli.command()
@click.option("--host", default="localhost", help="Ollama í˜¸ìŠ¤íŠ¸")
@click.option("--port", default=11434, help="Ollama í¬íŠ¸")
def check(host: str, port: int):
    """Ollama ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    import ollama

    click.echo(f"ğŸ”Œ Checking Ollama connection: {host}:{port}")

    try:
        # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        result = ollama.list()
        models = result.get("models", [])

        click.echo(f"âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
        click.echo(f"\nì„¤ì¹˜ëœ ëª¨ë¸ ({len(models)}ê°œ):")

        for model in models:
            name = model.get("name", "unknown")
            size = model.get("size", 0) / (1024**3)  # GB
            click.echo(f"  - {name} ({size:.1f} GB)")

        # í•„ìš”í•œ ëª¨ë¸ í™•ì¸
        model_names = [m.get("name") for m in models]

        click.echo(f"\ní•„ìˆ˜ ëª¨ë¸ í™•ì¸:")
        embedding_ok = settings.OLLAMA_EMBEDDING_MODEL in model_names
        llm_ok = settings.OLLAMA_LLM_MODEL in model_names

        click.echo(f"  Embedding ({settings.OLLAMA_EMBEDDING_MODEL}): {'âœ…' if embedding_ok else 'âŒ'}")
        click.echo(f"  LLM ({settings.OLLAMA_LLM_MODEL}): {'âœ…' if llm_ok else 'âŒ'}")

        if not (embedding_ok and llm_ok):
            click.echo(f"\nâš ï¸  í•„ìš”í•œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            if not embedding_ok:
                click.echo(f"   $ ollama pull {settings.OLLAMA_EMBEDDING_MODEL}")
            if not llm_ok:
                click.echo(f"   $ ollama pull {settings.OLLAMA_LLM_MODEL}")

    except Exception as e:
        click.echo(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        click.echo(f"\nğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:")
        click.echo(f"   $ ollama serve")


def main():
    """Main entry point."""
    cli()


if __name__ == "__main__":
    main()
